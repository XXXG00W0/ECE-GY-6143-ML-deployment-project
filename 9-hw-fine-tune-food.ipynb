{"cells":[{"cell_type":"markdown","metadata":{"id":"7h7ocCb3FLzh"},"source":["# Transfer learning: classifying food items\n","\n","_Fraida Fund_\n"]},{"cell_type":"markdown","metadata":{"id":"5xgIsiLdJzEu"},"source":["* Name: Ziyi Liang\n","* Net ID: zl5604"]},{"cell_type":"markdown","metadata":{"id":"ETjSdnVFFPM1"},"source":["This week, we practiced using transfer learning to classify hand shapes as rock, paper, or scissors.\n","\n","But that was a relatively easy problem. In this notebook, we'll work on classifying photographs of food items into 11 classes: bread, dairy product, dessert, egg, fried food, meat, noodles/pasta, rice, seafood, soup, and vegetable/fruit.\n","\n","This is a more challenging problem, because there is a lot of intra-class variability (e.g. different fruits and vegetables don't have a lot in common) and also some inter-class similarity (e.g. a bowl of rice can look similar to a bown of pasta).\n","\n","To see what we mean by these, let's look at some examples of food photographs from the dataset that we'll work with.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BSMGtDTDPgVz"},"source":["Note: you can refer to this week's demo notebook for a closely related example.\n","\n","Note: you may want to change the notebook's runtime type to GPU, for faster model training.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9IM-htbFH4lv"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","from PIL import Image\n","\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"YruaZAUWkPbu"},"source":["## Get data\n","\n","We are going to use the [Food-11 dataset](https://www.epfl.ch/labs/mmspg/downloads/food-image-datasets/), from the EPFL Multimedia Signal Processing Group. Here's a description:\n","\n","> This is a dataset containing 16643 food images grouped in 11 major food categories. The 11 categories are Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit. Similar as Food-5K dataset, the whole dataset is divided in three parts: training, validation and evaluation. The same naming convention is used, where ID 0-10 refers to the 11 food categories respectively.\n",">\n","> The total file size of the Food-11 dataset is about 1.16 GB.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aRCiSpBJGnjU"},"outputs":[],"source":["classes = np.array([\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\",\n","\t\"Meat\", \"Noodles/Pasta\", \"Rice\", \"Seafood\", \"Soup\",\n","\t\"Vegetable/Fruit\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-6AL3EijixD"},"outputs":[],"source":["!mkdir -p /content/Food-11\n","%cd /content/Food-11\n","!gdown https://drive.google.com/uc?id=1dt3CD3ICdLbTf80sNJ25TPBDKu_qyCnq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbPzy4Wdj7v1"},"outputs":[],"source":["!unzip Food-11.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3P0GDcTI2dff"},"outputs":[],"source":["%cd /content"]},{"cell_type":"markdown","metadata":{"id":"Z6IJBiGYoV3Z"},"source":["Click on the folder icon in Colab to view your filesystem and verify that you have the `Food-11` directory with `training`, `validation`, and `evaluation` subdirectories."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2YmOs_NQlSAA"},"outputs":[],"source":["training_dir = \"Food-11/training/\"\n","# training_images = os.listdir(training_dir)\n","# training_labels = np.array([int(f.split('_')[0]) for f in training_images ])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QRuh8Ge0HIWl"},"outputs":[],"source":["validation_dir = \"Food-11/validation/\"\n","# validation_images = os.listdir(validation_dir)\n","# validation_labels = np.array([int(f.split('_')[0]) for f in validation_images ])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VOnaBYJxnLpF"},"outputs":[],"source":["evaluation_dir = \"Food-11/evaluation/\"\n","# evaluation_images = os.listdir(evaluation_dir)\n","# evaluation_labels = np.array([int(f.split('_')[0]) for f in evaluation_images ])"]},{"cell_type":"markdown","metadata":{"id":"7AQePmXSlD0A"},"source":["Let's look at a few random training samples:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v0vaPnfYls15"},"outputs":[],"source":["num_classes = len(classes)\n","samples_per_class = 4\n","figure = plt.figure(figsize=(num_classes*2,samples_per_class*2))\n","for cls_idx, cls in enumerate(classes):\n","  idxs = np.flatnonzero(training_labels == cls_idx)\n","  idxs = np.random.choice(idxs, samples_per_class, replace=False)\n","  for i, idx in enumerate(idxs):\n","    plt_idx = i * num_classes + cls_idx + 1\n","    plt.subplot(samples_per_class, num_classes, plt_idx)\n","    im = Image.open(training_dir + training_images[idx])\n","    plt.imshow(im)\n","    plt.axis('off')\n","    if i == 0:\n","      plt.title(cls)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VshTPRiaGvhH"},"source":["We can see that two dessert samples (for example) may look very different from one another - intra-class variability. Furthermore, samples from two different classes, such as a rice dish and a noodles dish, may look very similar to one another - inter-class similarity.\n","\n","(In fact, some foods may really belong to multiple classes.  For example, fried chicken should really have both the \"fried food\" and \"meat\" labels, but only one class label is provided for each sample in the dataset.)"]},{"cell_type":"markdown","metadata":{"id":"GZ3owlgmnhVH"},"source":["## Prepare data directories"]},{"cell_type":"markdown","metadata":{"id":"xuj_nPRSnjHX"},"source":["Because the Food-11 dataset is very large, we won't want to load the entire dataset into memory at once.\n","\n","Instead, Keras will let us read in images from disk as they are needed (in batches). For this to work, Keras [expects](https://keras.io/api/preprocessing/) that images will be organized in the following directory structure, with one subdirectory per class:\n","\n","```\n","main_directory/\n","...class_a/\n","......a_image_1.jpg\n","......a_image_2.jpg\n","...class_b/\n","......b_image_1.jpg\n","......b_image_2.jpg\n","```\n","\n","So, we will have to re-organize our directory structure to match."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"03sy3JMHpX-g"},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\\Food-11\\training\n"]}],"source":["# %cd /content/Food-11/training/\n","%cd \"D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\\Food-11\\training\"\n","\n","# loop over classes\n","for i in range(len(classes)):\n","  # make a directory for this class inside Food-11/training\n","  try:\n","    os.mkdir(\"class_%02d\" % i)\n","  except FileExistsError:\n","    pass\n","  # get list of files inside Food-11/training that have this class label\n","  # files = [f for f in os.listdir('/content/Food-11/training/') if f.startswith(\"%d_\" % i)]\n","  files = [f for f in os.listdir('.') if f.startswith(\"%d_\" % i)]\n","  # move each file to the subdirectory for the class\n","  for f in files:\n","    shutil.move(f, \"class_%02d/\" % i)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PgWSP55SADyQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\\Food-11\\validation\n"]}],"source":["# %cd /content/Food-11/validation/\n","%cd \"D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\\Food-11\\validation\"\n","\n","\n","for i in range(len(classes)):\n","  try:\n","    os.mkdir(\"class_%02d\" % i)\n","  except FileExistsError:\n","    pass\n","  # files = [f for f in os.listdir('/content/Food-11/validation/') if f.startswith(\"%d_\" % i)]\n","  files = [f for f in os.listdir('.') if f.startswith(\"%d_\" % i)]\n","  for f in files:\n","    shutil.move(f, \"class_%02d/\" % i)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nVB-XSeiHeWO"},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\\Food-11\\evaluation\n"]}],"source":["# %cd /content/Food-11/evaluation/\n","%cd \"D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\\Food-11\\evaluation\"\n","\n","\n","for i in range(len(classes)):\n","  try:\n","    os.mkdir(\"class_%02d\" % i)\n","  except FileExistsError:\n","    pass\n","  # files = [f for f in os.listdir('/content/Food-11/evaluation/') if f.startswith(\"%d_\" % i)]\n","  files = [f for f in os.listdir('.') if f.startswith(\"%d_\" % i)]\n","  for f in files:\n","    shutil.move(f, \"class_%02d/\" % i)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Z25kVS6l-DHC"},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\n"]}],"source":["# go back to default working directory\n","# %cd /content\n","%cd \"D:\\文档\\NYU Classes\\2023 Fall\\ECE-GY 6143 Machine Learning\\Lab 11\""]},{"cell_type":"markdown","metadata":{"id":"S5ZQP_nyHuQr"},"source":["Use the folder icon in Colab to look at your directory structure, and verify that within the training, validation, and evaluation data directories, there are subdirectories for each class containing the samples from that class."]},{"cell_type":"markdown","metadata":{"id":"67y_fWMtHoXu"},"source":["## Prepare data generators"]},{"cell_type":"markdown","metadata":{"id":"MvzQTiVcHq5y"},"source":["Now that we have prepared the directory structure, we can set up our data \"flow\"."]},{"cell_type":"markdown","metadata":{"id":"6WH-aWQLIAwB"},"source":["In the next cell, I have defined a batch size and an image size. You are free to change these parameters, or you can keep them. You can use any batch size that has good results in training, and you can change the input image size to match the input shape of whatever base model you are using."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9O8lRNNW9vNh"},"outputs":[],"source":["# TODO 1 (optional) - design choice on batch size, image shape\n","BATCH_SIZE=32\n","INPUT_IMG_SIZE = 224"]},{"cell_type":"markdown","metadata":{"id":"g1QeYrmXIP41"},"source":["Then, I have defined some data generators to create augmented samples (slightly changed versions of the original samples). You are free to change the transformations used to generate the augmented data. You can learn more about the image transformations in the [ImageDataGenerator documentation](https://keras.io/api/preprocessing/image/#imagedatagenerator-class).\n","\n","After we define the data generator, we use  `flow_from_directory` to get images off the disk during training, on an as-needed basis."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"4N_1MsLCtMSS"},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] 系统找不到指定的路径。: '/content/Food-11/training/'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m training_aug \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mImageDataGenerator(\n\u001b[0;32m      5\u001b[0m   rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,          \u001b[38;5;66;03m# rescales so each pixel is in 0-1 range\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \trotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \thorizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m \tfill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# prepare generator that pulls images from directory (and resizes)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m training_gen \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_aug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mINPUT_IMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINPUT_IMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\hower\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:1650\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1566\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1580\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1581\u001b[0m ):\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m \n\u001b[0;32m   1584\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;124;03m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\hower\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/content/Food-11/training/'"]}],"source":["# TODO 2 (optional) - design choices on image transformations for data augmentation\n","\n","# prepare ImageDataGenerator to create augmented training samples\n","training_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n","  rescale=1.0/255,          # rescales so each pixel is in 0-1 range\n","\trotation_range=1,\n","\tzoom_range=0.1,\n","\twidth_shift_range=0.1,\n","\theight_shift_range=0.1,\n","\tshear_range=0.1,\n","\thorizontal_flip=True,\n","\tfill_mode=\"nearest\")\n","\n","# prepare generator that pulls images from directory (and resizes)\n","training_gen = training_aug.flow_from_directory(\n","\t'/content/' + training_dir,\n","\ttarget_size=(INPUT_IMG_SIZE, INPUT_IMG_SIZE),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=True,\n","\tbatch_size=BATCH_SIZE,\n","  class_mode='sparse')"]},{"cell_type":"markdown","metadata":{"id":"9d4aWBAhhMLv"},"source":["Check the mapping of directory names to class labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ogq9hl-g9EN"},"outputs":[],"source":["training_gen.class_indices"]},{"cell_type":"markdown","metadata":{"id":"AUUcSa-GLEYt"},"source":["We also set up the validation and evaluation (test) sets in a similar manner:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xlt29lyI_01x"},"outputs":[],"source":["validation_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rescale=1.0/255\n",")\n","\n","validation_gen = validation_aug.flow_from_directory(\n","\t'/content/' + validation_dir,\n","\ttarget_size=(INPUT_IMG_SIZE, INPUT_IMG_SIZE),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=True,\n","\tbatch_size=BATCH_SIZE,\n","  class_mode='sparse')"]},{"cell_type":"markdown","metadata":{"id":"icpl6cxvitby"},"source":["Don't shuffle the evaluation set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6o10Cp7ELHhd"},"outputs":[],"source":["evaluation_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rescale=1.0/255\n",")\n","\n","evaluation_gen = evaluation_aug.flow_from_directory(\n","\t'/content/' + evaluation_dir,\n","\ttarget_size=(INPUT_IMG_SIZE, INPUT_IMG_SIZE),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=False,\n","\tbatch_size=BATCH_SIZE,\n","  class_mode='sparse')"]},{"cell_type":"markdown","metadata":{"id":"_LYHGv-E-pwd"},"source":["## Prepare base model"]},{"cell_type":"markdown","metadata":{"id":"eJzQMcB0LVZ-"},"source":["Now that the data is ready, we need to prepare the model.\n"]},{"cell_type":"markdown","metadata":{"id":"iae63Zl4LaS2"},"source":["First, review the models available as [Keras Applications](https://keras.io/api/applications/), and decide which model you will use. Make sure to click through from the table to the model documentation, to learn more about each model you are considering."]},{"cell_type":"markdown","metadata":{"id":"gZb6RFAGMdCO"},"source":["In the cell that follows, I have used the [VGG16](https://keras.io/api/applications/vgg/) model, but you can change that - you may use any model that you think will have good results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ijocY-Q-q3Y"},"outputs":[],"source":["# TODO 3 (optional) - design choice on base model\n","\n","base_model = tf.keras.applications.VGG16(\n","  input_shape=(INPUT_IMG_SIZE,INPUT_IMG_SIZE,3),\n","  include_top=False,\n","  pooling='avg'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzPHwsni_OOY"},"outputs":[],"source":["base_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"XaSejES8OXs1"},"source":["## Train model with new classification head"]},{"cell_type":"markdown","metadata":{"id":"sCuIodDBNFsD"},"source":["In the following cell, \"freeze\" your base model (set it so that it is not trainable), and then construct a new model using the frozen base model with a new classification head.  (You can add a dropout layer in between if you think it will be helpful.)\n","\n","(You can refer to this week's demo notebook for an example.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9S4wHbN_lC4"},"outputs":[],"source":["# TODO 4 (required) - prepare model with new classification head\n","\n","model = tf.keras.models.Sequential()\n","# add to your model here...\n"]},{"cell_type":"markdown","metadata":{"id":"4ydUj8ndNe9Y"},"source":["Then, print a model summary:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLOgq4V0_wAH"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Bs3tRO09N_B3"},"source":["(Make sure that most parameters are \"non-trainable\", because you have frozen the base model!)"]},{"cell_type":"markdown","metadata":{"id":"3Hj7TU8QNtHQ"},"source":["Compile your model with an appropriate optimizer and loss function, and use accuracy as a metric."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fq29iyDD_xsp"},"outputs":[],"source":["# TODO 5 (required) - design training parameters, and compile\n","\n","# model.compile(...)\n"]},{"cell_type":"markdown","metadata":{"id":"FKhOiBOhN44B"},"source":["Finally, fit your model with the new classification head for as many epochs as you think is appropriate. You may also use early stopping if you think it will be helpful.\n","\n","Pass the validation data generator as `validation_data`.\n","\n","(You can refer to this week's demo notebook for an example.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5c-eT2xZANiP"},"outputs":[],"source":["# TODO 6 (required) - fit model, you decide how many epochs\n","# note: you can get the number of training samples with training_gen.n\n","# and the number of validation samples with validation_gen.n\n","\n","# n_epochs = ...\n","# hist = model.fit(...)\n"]},{"cell_type":"markdown","metadata":{"id":"QpALQQS5PnVG"},"source":["Plot the training history. In one subplot, show loss vs. epoch for the training and validation sets on the same plot. In the second subplot, show accuracy vs. epoch for the training and validation sets on the same plot."]},{"cell_type":"markdown","metadata":{"id":"zuN1eFlnYliH"},"source":["Make sure you show the training history from the *beginning* of training. (If you re-run the `hist = fit(...)` command, you'll overwrite the previous history and lose the training history from the beginning, so be careful not to do that in your final submission!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHRrGZuxPqdt"},"outputs":[],"source":["# TODO 7 (required) - plot training history\n"]},{"cell_type":"markdown","metadata":{"id":"8zcjS3zLOaj8"},"source":["## Fine-tune model"]},{"cell_type":"markdown","metadata":{"id":"MIvVbZ5jOeS_"},"source":["Next, we are going to \"un-freeze\" the later layers of the model, and train it for a few more epochs on our data (with a smaller learning rate), so that it is better suited for our specific classification task."]},{"cell_type":"markdown","metadata":{"id":"vpKQzY0iQwMf"},"source":["Note that you should *not* create a new model. We're just going to continue training the model we already started training."]},{"cell_type":"markdown","metadata":{"id":"Cu5MMmW3Tqu9"},"source":["First, we will un-freeze the base model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zvH5cbUTlce"},"outputs":[],"source":["base_model.trainable = True"]},{"cell_type":"markdown","metadata":{"id":"oE1jagiiTsdb"},"source":["Then, we will re-freeze only the first layers of the base model. In the cell below, I have left the last five layers, which includes the last set of convolutional filters, unfrozen. But you are free to change this number if you think you will get better performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0Nzlw-jRF6L"},"outputs":[],"source":["# TODO 8 (optional) - you can decide how many layers to unfreeze\n","\n","for layer in base_model.layers[:-5]:\n","\tlayer.trainable = False"]},{"cell_type":"markdown","metadata":{"id":"Mg5eufJMRX26"},"source":["The output of the following cell will indicate which layers in the base model are trainable, and which are not:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXj5rAKfRa_H"},"outputs":[],"source":["for layer in base_model.layers:\n","  print(\"{}: {}\".format(layer, layer.trainable))"]},{"cell_type":"markdown","metadata":{"id":"Ei-B5vBqUYAb"},"source":["Also, the model summary should now show more trainable parameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YASFan3RqYD"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"yOZYyBqiUb1t"},"source":["Re-compile your model. Don't forget to use a smaller learning rate for fine-tuning! We don't want to make major changes to the last feature extraction layers, we just want to adjust them a little bit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9ZeQWf0RxOV"},"outputs":[],"source":["# TODO 9 (required) - re-compile model\n","# use a smaller learning rate for fine-tuning\n","\n","# model.compile(...)\n"]},{"cell_type":"markdown","metadata":{"id":"y6QKGEeNUrkW"},"source":["Fit your model (with some newly unfrozen layers) for some more epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGtHqsZuU0M2"},"outputs":[],"source":["# TODO 10 (required) - fit model, you decide how many epochs\n","# note: you can get the number of training samples with training_gen.n\n","# and the number of validation samples with validation_gen.n\n","\n","# n_epochs_fine = ...\n","# hist_fine = model.fit(...)\n"]},{"cell_type":"markdown","metadata":{"id":"Vw5OSPJAVIc-"},"source":["Plot the complete training history. In one subplot, show loss vs. epoch for the training and validation sets on the same plot. In the second subplot, show accuracy vs. epoch for the training and validation sets on the same plot. Draw a vertical line demarcating the first stage of training (training the classification head) and the second stage of training (fine tuning).\n","\n","(You can refer to this week's demo notebook for an example.)"]},{"cell_type":"markdown","metadata":{"id":"to2F-yESYwdN"},"source":["Make sure you show the training history from the *beginning* of training. (If you re-run the `hist_fine = fit(...)` command, you'll overwrite the previous history and lose the training history from the beginning, so be careful not to do that in your final submission!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBnfw72ZVYNz"},"outputs":[],"source":["# TODO 11 (required) - plot training history\n"]},{"cell_type":"markdown","metadata":{"id":"SZWO3ILaOjsr"},"source":["## Evaluate model performance\n"]},{"cell_type":"markdown","metadata":{"id":"eXMmLhC0XgNq"},"source":["Once you have finished training your model, use `evaluate` to get the model performance - loss and accuracy - on the evaluation set. (You should not use the evaluation set at all prior to this step.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioyIKe0DXUz7"},"outputs":[],"source":["model.evaluate(evaluation_gen)"]},{"cell_type":"markdown","metadata":{"id":"vkMrM7tLbv4e"},"source":["We can also create a confusion matrix, to see which which classes are most often confused with one another."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bytoNNyLeJjL"},"outputs":[],"source":["y_pred_prob = model.predict(evaluation_gen)\n","y_pred = np.argmax(y_pred_prob, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzHPEM9oYRD6"},"outputs":[],"source":["cm = pd.crosstab(evaluation_gen.classes, y_pred,\n","                               rownames=['Actual'], colnames=['Predicted'],\n","                               normalize='index')\n","p = plt.figure(figsize=(10,10));\n","p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False);\n","p = plt.xticks(ticks=np.arange(0,11), labels=classes, rotation=45)\n","p = plt.yticks(ticks=np.arange(0,11), labels=classes, rotation=45)"]},{"cell_type":"markdown","metadata":{"id":"G3iA5I5FaeBJ"},"source":["## Save your model"]},{"cell_type":"markdown","metadata":{"id":"nzxuNXhGai6r"},"source":["Use the following cell to save your model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvKTAkDladKj"},"outputs":[],"source":["model.save(\"model.keras\")"]},{"cell_type":"markdown","metadata":{"id":"-p12O802axbj"},"source":["Then, download it for later use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRPAgf57bKjF"},"outputs":[],"source":["from google.colab import files\n","files.download('model.keras')"]},{"cell_type":"markdown","metadata":{"id":"vu-O0yJTOlEq"},"source":["## Use model on your own custom image"]},{"cell_type":"markdown","metadata":{"id":"q_mKCQ0hWwjV"},"source":["Take your own photograph of a food item in one of the 11 classes. Upload it to Colab, load it as a numpy array, and process the image (resize it, and scale so that each pixel takes on a value from 0 to 1).\n","\n","Then, use the fitted model to get the per-class probabilities for each class.\n","\n","\n","Plot your image, and a bar plot showing the probability for the 5 most likely classes according to the model.\n","\n","(You can refer to the \"Classify with MobileNetV2\" section of this week's notebook for an example.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3Cg07rZXNVR"},"outputs":[],"source":["# TODO 12 (required) - show example of model use on a new image"]},{"cell_type":"markdown","metadata":{"id":"3lkVkl-phXxg"},"source":["## Describe your model - TODO 13 (required)\n","\n","In the following cell, describe your final model and your results, by answering the following questions.\n","\n","* What base model did you use? How many parameters does this base model have?\n","* Describe the specific transformations you used to create an augmented data set.\n","* For how many epochs did you train your classification head, and with what optimizer, learning rate, and batch size? What was the validation accuracy at the end of this training stage?\n","* Which layers did you un-freeze for fine-tuning, how many epochs did you fine-tune for, and what optimizer, learning rate, and batch size did you use for fine-tuning? What was the validation accuracy at the end of this training stage?\n","* What was your final accuracy on the evaluation set?\n","* Did your model correctly predict the class of your custom test image?"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["_LYHGv-E-pwd","XaSejES8OXs1","8zcjS3zLOaj8","3lkVkl-phXxg"],"provenance":[{"file_id":"16w-mLZ4tSxwH7bZof-1Baota-TIYv19B","timestamp":1701818584237}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
