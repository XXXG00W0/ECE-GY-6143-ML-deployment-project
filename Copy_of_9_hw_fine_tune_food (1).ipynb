{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h7ocCb3FLzh"
      },
      "source": [
        "# Transfer learning: classifying food items\n",
        "\n",
        "_Fraida Fund_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xgIsiLdJzEu"
      },
      "source": [
        "* Name: Yihua Yang\n",
        "* Net ID: yy5028"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETjSdnVFFPM1"
      },
      "source": [
        "This week, we practiced using transfer learning to classify hand shapes as rock, paper, or scissors.\n",
        "\n",
        "But that was a relatively easy problem. In this notebook, we'll work on classifying photographs of food items into 11 classes: bread, dairy product, dessert, egg, fried food, meat, noodles/pasta, rice, seafood, soup, and vegetable/fruit.\n",
        "\n",
        "This is a more challenging problem, because there is a lot of intra-class variability (e.g. different fruits and vegetables don't have a lot in common) and also some inter-class similarity (e.g. a bowl of rice can look similar to a bown of pasta).\n",
        "\n",
        "To see what we mean by these, let's look at some examples of food photographs from the dataset that we'll work with.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMGtDTDPgVz"
      },
      "source": [
        "Note: you can refer to this week's demo notebook for a closely related example.\n",
        "\n",
        "Note: you may want to change the notebook's runtime type to GPU, for faster model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9IM-htbFH4lv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YruaZAUWkPbu"
      },
      "source": [
        "## Get data\n",
        "\n",
        "We are going to use the [Food-11 dataset](https://www.epfl.ch/labs/mmspg/downloads/food-image-datasets/), from the EPFL Multimedia Signal Processing Group. Here's a description:\n",
        "\n",
        "> This is a dataset containing 16643 food images grouped in 11 major food categories. The 11 categories are Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit. Similar as Food-5K dataset, the whole dataset is divided in three parts: training, validation and evaluation. The same naming convention is used, where ID 0-10 refers to the 11 food categories respectively.\n",
        ">\n",
        "> The total file size of the Food-11 dataset is about 1.16 GB.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aRCiSpBJGnjU"
      },
      "outputs": [],
      "source": [
        "classes = np.array([\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\",\n",
        "\t\"Meat\", \"Noodles/Pasta\", \"Rice\", \"Seafood\", \"Soup\",\n",
        "\t\"Vegetable/Fruit\"])\n",
        "root_dir=r\"C:\\Users\\zaqx5\\Downloads\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-6AL3EijixD",
        "outputId": "601cea2a-fadb-4f33-f047-a6d86415ecc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] 系统找不到指定的路径。: '/content/Food-11'\n",
            "c:\\Users\\zaqx5\\Downloads\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "�����﷨����ȷ��\n",
            "'gdown' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ./content/Food-11\n",
        "%cd /content/Food-11\n",
        "!gdown https://drive.google.com/uc?id=1dt3CD3ICdLbTf80sNJ25TPBDKu_qyCnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbPzy4Wdj7v1",
        "outputId": "05eb584a-7ad0-4ccb-be77-5cb4a6ace845"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n"
          ]
        }
      ],
      "source": [
        "!unzip Food-11.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P0GDcTI2dff",
        "outputId": "392bf33b-78c5-433d-b794-f8dfd47847e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zaqx5\\Downloads\n"
          ]
        }
      ],
      "source": [
        "%cd ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6IJBiGYoV3Z"
      },
      "source": [
        "Click on the folder icon in Colab to view your filesystem and verify that you have the `Food-11` directory with `training`, `validation`, and `evaluation` subdirectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2YmOs_NQlSAA"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'class'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m training_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood-11/training/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m training_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(training_dir)\n\u001b[1;32m----> 3\u001b[0m training_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m training_images ])\n",
            "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m training_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood-11/training/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m training_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(training_dir)\n\u001b[1;32m----> 3\u001b[0m training_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m training_images ])\n",
            "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'class'"
          ]
        }
      ],
      "source": [
        "training_dir = \"Food-11/training/\"\n",
        "training_images = os.listdir(training_dir)\n",
        "training_labels = np.array([int(f.split('_')[0]) for f in training_images ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QRuh8Ge0HIWl"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'class'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m validation_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood-11/validation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m validation_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(validation_dir)\n\u001b[1;32m----> 3\u001b[0m validation_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m validation_images ])\n",
            "Cell \u001b[1;32mIn[13], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m validation_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood-11/validation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m validation_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(validation_dir)\n\u001b[1;32m----> 3\u001b[0m validation_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m validation_images ])\n",
            "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'class'"
          ]
        }
      ],
      "source": [
        "validation_dir = \"Food-11/validation/\"\n",
        "validation_images = os.listdir(validation_dir)\n",
        "validation_labels = np.array([int(f.split('_')[0]) for f in validation_images ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VOnaBYJxnLpF"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'class'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m evaluation_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood-11/evaluation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m evaluation_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(evaluation_dir)\n\u001b[1;32m----> 3\u001b[0m evaluation_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m evaluation_images ])\n",
            "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m evaluation_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood-11/evaluation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m evaluation_images \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(evaluation_dir)\n\u001b[1;32m----> 3\u001b[0m evaluation_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m evaluation_images ])\n",
            "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'class'"
          ]
        }
      ],
      "source": [
        "evaluation_dir = \"Food-11/evaluation/\"\n",
        "evaluation_images = os.listdir(evaluation_dir)\n",
        "evaluation_labels = np.array([int(f.split('_')[0]) for f in evaluation_images ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AQePmXSlD0A"
      },
      "source": [
        "Let's look at a few random training samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "v0vaPnfYls15",
        "outputId": "f29cfa7a-cc58-4aae-bfab-38091d6f4ba6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'training_labels' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m figure \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(num_classes\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,samples_per_class\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cls_idx, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes):\n\u001b[1;32m----> 5\u001b[0m   idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflatnonzero(\u001b[43mtraining_labels\u001b[49m \u001b[38;5;241m==\u001b[39m cls_idx)\n\u001b[0;32m      6\u001b[0m   idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(idxs, samples_per_class, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idxs):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'training_labels' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 2200x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_classes = len(classes)\n",
        "samples_per_class = 4\n",
        "figure = plt.figure(figsize=(num_classes*2,samples_per_class*2))\n",
        "for cls_idx, cls in enumerate(classes):\n",
        "  idxs = np.flatnonzero(training_labels == cls_idx)\n",
        "  idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "  for i, idx in enumerate(idxs):\n",
        "    plt_idx = i * num_classes + cls_idx + 1\n",
        "    plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "    im = Image.open(training_dir + training_images[idx])\n",
        "    plt.imshow(im)\n",
        "    plt.axis('off')\n",
        "    if i == 0:\n",
        "      plt.title(cls)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VshTPRiaGvhH"
      },
      "source": [
        "We can see that two dessert samples (for example) may look very different from one another - intra-class variability. Furthermore, samples from two different classes, such as a rice dish and a noodles dish, may look very similar to one another - inter-class similarity.\n",
        "\n",
        "(In fact, some foods may really belong to multiple classes.  For example, fried chicken should really have both the \"fried food\" and \"meat\" labels, but only one class label is provided for each sample in the dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ3owlgmnhVH"
      },
      "source": [
        "## Prepare data directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuj_nPRSnjHX"
      },
      "source": [
        "Because the Food-11 dataset is very large, we won't want to load the entire dataset into memory at once.\n",
        "\n",
        "Instead, Keras will let us read in images from disk as they are needed (in batches). For this to work, Keras [expects](https://keras.io/api/preprocessing/) that images will be organized in the following directory structure, with one subdirectory per class:\n",
        "\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_image_1.jpg\n",
        "......a_image_2.jpg\n",
        "...class_b/\n",
        "......b_image_1.jpg\n",
        "......b_image_2.jpg\n",
        "```\n",
        "\n",
        "So, we will have to re-organize our directory structure to match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03sy3JMHpX-g",
        "outputId": "38d3f1a3-faa1-435c-e543-3929530dd1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zaqx5\\Downloads\n",
            "C:\\Users\\zaqx5\\Downloads\\Food-11\\training\n"
          ]
        }
      ],
      "source": [
        "%cd $root_dir\n",
        "%cd ./Food-11/training/\n",
        "\n",
        "# loop over classes\n",
        "for i in range(len(classes)):\n",
        "  # make a directory for this class inside Food-11/training\n",
        "  try:\n",
        "    os.mkdir(\"class_%02d\" % i)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  # get list of files inside Food-11/training that have this class label\n",
        "  files = [f for f in os.listdir('.') if f.startswith(\"%d_\" % i)]\n",
        "  # move each file to the subdirectory for the class\n",
        "  for f in files:\n",
        "    shutil.move(f, \"class_%02d/\" % i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgWSP55SADyQ",
        "outputId": "87fd6186-200d-4a30-90f3-d8e835fee33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zaqx5\\Downloads\n",
            "C:\\Users\\zaqx5\\Downloads\\Food-11\\validation\n"
          ]
        }
      ],
      "source": [
        "%cd $root_dir\n",
        "%cd ./Food-11/validation/\n",
        "\n",
        "for i in range(len(classes)):\n",
        "  try:\n",
        "    os.mkdir(\"class_%02d\" % i)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  files = [f for f in os.listdir('.') if f.startswith(\"%d_\" % i)]\n",
        "  for f in files:\n",
        "    shutil.move(f, \"class_%02d/\" % i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVB-XSeiHeWO",
        "outputId": "799a5c47-8792-4b58-f06f-f5e92d6a8e6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zaqx5\\Downloads\n",
            "C:\\Users\\zaqx5\\Downloads\\Food-11\\evaluation\n"
          ]
        }
      ],
      "source": [
        "%cd $root_dir\n",
        "%cd ./Food-11/evaluation/\n",
        "\n",
        "for i in range(len(classes)):\n",
        "  try:\n",
        "    os.mkdir(\"class_%02d\" % i)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  files = [f for f in os.listdir('.') if f.startswith(\"%d_\" % i)]\n",
        "  for f in files:\n",
        "    shutil.move(f, \"class_%02d/\" % i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z25kVS6l-DHC",
        "outputId": "3e42b201-0c52-4866-fce0-bf1d020ff90f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zaqx5\\Downloads\n"
          ]
        }
      ],
      "source": [
        "# go back to default working directory\n",
        "%cd $root_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ZQP_nyHuQr"
      },
      "source": [
        "Use the folder icon in Colab to look at your directory structure, and verify that within the training, validation, and evaluation data directories, there are subdirectories for each class containing the samples from that class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67y_fWMtHoXu"
      },
      "source": [
        "## Prepare data generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvzQTiVcHq5y"
      },
      "source": [
        "Now that we have prepared the directory structure, we can set up our data \"flow\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WH-aWQLIAwB"
      },
      "source": [
        "In the next cell, I have defined a batch size and an image size. You are free to change these parameters, or you can keep them. You can use any batch size that has good results in training, and you can change the input image size to match the input shape of whatever base model you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9O8lRNNW9vNh"
      },
      "outputs": [],
      "source": [
        "# TODO 1 (optional) - design choice on batch size, image shape\n",
        "BATCH_SIZE=32\n",
        "INPUT_IMG_SIZE = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1QeYrmXIP41"
      },
      "source": [
        "Then, I have defined some data generators to create augmented samples (slightly changed versions of the original samples). You are free to change the transformations used to generate the augmented data. You can learn more about the image transformations in the [ImageDataGenerator documentation](https://keras.io/api/preprocessing/image/#imagedatagenerator-class).\n",
        "\n",
        "After we define the data generator, we use  `flow_from_directory` to get images off the disk during training, on an as-needed basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N_1MsLCtMSS",
        "outputId": "3e190402-ee5d-4b7c-ab5e-e454ceedd96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\zaqx5\\Downloads\n",
            "Found 9866 images belonging to 11 classes.\n"
          ]
        }
      ],
      "source": [
        "# TODO 2 (optional) - design choices on image transformations for data augmentation\n",
        "%cd $root_dir\n",
        "# prepare ImageDataGenerator to create augmented training samples\n",
        "training_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "  rescale=1.0/255,          # rescales so each pixel is in 0-1 range\n",
        "\trotation_range=1,\n",
        "\tzoom_range=0.1,\n",
        "\twidth_shift_range=0.1,\n",
        "\theight_shift_range=0.1,\n",
        "\tshear_range=0.1,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# prepare generator that pulls images from directory (and resizes)\n",
        "training_gen = training_aug.flow_from_directory(\n",
        "\ttraining_dir,\n",
        "\ttarget_size=(INPUT_IMG_SIZE, INPUT_IMG_SIZE),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=BATCH_SIZE,\n",
        "  class_mode='sparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4aWBAhhMLv"
      },
      "source": [
        "Check the mapping of directory names to class labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ogq9hl-g9EN",
        "outputId": "fdf86c4e-a0f6-40d0-8cb7-943d5b3f4e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9866,)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_gen.class_indices\n",
        "training_gen.classes.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUUcSa-GLEYt"
      },
      "source": [
        "We also set up the validation and evaluation (test) sets in a similar manner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlt29lyI_01x",
        "outputId": "0120efcc-ea23-46b4-c521-1db2a40e1fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3430 images belonging to 11 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1.0/255\n",
        ")\n",
        "\n",
        "validation_gen = validation_aug.flow_from_directory(\n",
        "\tvalidation_dir,\n",
        "\ttarget_size=(INPUT_IMG_SIZE, INPUT_IMG_SIZE),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=BATCH_SIZE,\n",
        "  class_mode='sparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icpl6cxvitby"
      },
      "source": [
        "Don't shuffle the evaluation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o10Cp7ELHhd",
        "outputId": "e20ece32-8945-4a91-fda3-f08c1cb0ae8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3347 images belonging to 11 classes.\n"
          ]
        }
      ],
      "source": [
        "evaluation_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1.0/255\n",
        ")\n",
        "\n",
        "evaluation_gen = evaluation_aug.flow_from_directory(\n",
        "\tevaluation_dir,\n",
        "\ttarget_size=(INPUT_IMG_SIZE, INPUT_IMG_SIZE),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=BATCH_SIZE,\n",
        "  class_mode='sparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LYHGv-E-pwd"
      },
      "source": [
        "## Prepare base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJzQMcB0LVZ-"
      },
      "source": [
        "Now that the data is ready, we need to prepare the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iae63Zl4LaS2"
      },
      "source": [
        "First, review the models available as [Keras Applications](https://keras.io/api/applications/), and decide which model you will use. Make sure to click through from the table to the model documentation, to learn more about each model you are considering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZb6RFAGMdCO"
      },
      "source": [
        "In the cell that follows, I have used the [VGG16](https://keras.io/api/applications/vgg/) model, but you can change that - you may use any model that you think will have good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ijocY-Q-q3Y",
        "outputId": "a4927656-5eb4-46b0-b310-9d04ba0f48c6"
      },
      "outputs": [],
      "source": [
        "# TODO 3 (optional) - design choice on base model\n",
        "\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "  input_shape=(INPUT_IMG_SIZE,INPUT_IMG_SIZE,3),\n",
        "  include_top=False,\n",
        "  pooling='avg'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzPHwsni_OOY",
        "outputId": "d5ac4fb8-5689-4e94-cae1-7f86a42a3b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaSejES8OXs1"
      },
      "source": [
        "## Train model with new classification head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCuIodDBNFsD"
      },
      "source": [
        "In the following cell, \"freeze\" your base model (set it so that it is not trainable), and then construct a new model using the frozen base model with a new classification head.  (You can add a dropout layer in between if you think it will be helpful.)\n",
        "\n",
        "(You can refer to this week's demo notebook for an example.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "h9S4wHbN_lC4"
      },
      "outputs": [],
      "source": [
        "# TODO 4 (required) - prepare model with new classification head\n",
        "base_model.trainable=False\n",
        "model = tf.keras.models.Sequential()\n",
        "# add to your model here...\n",
        "model.add(base_model)\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=num_classes,\n",
        "    activation=tf.keras.activations.softmax\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ydUj8ndNe9Y"
      },
      "source": [
        "Then, print a model summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLOgq4V0_wAH",
        "outputId": "105d4d58-bdbf-41a4-b86d-44fd245fe52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 512)               14714688  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11)                5643      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,720,331\n",
            "Trainable params: 5,643\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "model.summary()\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs3tRO09N_B3"
      },
      "source": [
        "(Make sure that most parameters are \"non-trainable\", because you have frozen the base model!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hj7TU8QNtHQ"
      },
      "source": [
        "Compile your model with an appropriate optimizer and loss function, and use accuracy as a metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Fq29iyDD_xsp"
      },
      "outputs": [],
      "source": [
        "# TODO 5 (required) - design training parameters, and compile\n",
        "\n",
        "# model.compile(...)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKhOiBOhN44B"
      },
      "source": [
        "Finally, fit your model with the new classification head for as many epochs as you think is appropriate. You may also use early stopping if you think it will be helpful.\n",
        "\n",
        "Pass the validation data generator as `validation_data`.\n",
        "\n",
        "(You can refer to this week's demo notebook for an example.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CtI_ktxoZsJP"
      },
      "outputs": [],
      "source": [
        "# reusing the callback function from lab 8\n",
        "from tensorflow.keras import callbacks\n",
        "# TODO - write a callback function\n",
        "class TrainToAccuracy(callbacks.Callback):\n",
        "\n",
        "    def __init__(self, threshold=0.9, patience=3):\n",
        "        self.threshold = threshold  # the desired accuracy threshold\n",
        "        self.patience = patience # how many epochs to wait once hitting the threshold\n",
        "        self.hit_count=0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_acc = logs.get(\"val_accuracy\")\n",
        "        # if conditions are met..\n",
        "        if current_acc>self.threshold:\n",
        "          self.hit_count+=1\n",
        "          if self.hit_count==self.patience:\n",
        "            self.model.stop_training = True\n",
        "            self.hit_count=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c-eT2xZANiP",
        "outputId": "4fc9d587-686f-4fe7-97be-7b39a983a5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "309/309 [==============================] - 48s 147ms/step - loss: 1.7574 - accuracy: 0.4131\n",
            "Epoch 2/200\n",
            "309/309 [==============================] - 45s 144ms/step - loss: 1.4028 - accuracy: 0.5260\n",
            "Epoch 3/200\n",
            "309/309 [==============================] - 45s 144ms/step - loss: 1.3036 - accuracy: 0.5596\n",
            "Epoch 4/200\n",
            "309/309 [==============================] - 44s 144ms/step - loss: 1.2563 - accuracy: 0.5713\n",
            "Epoch 5/200\n",
            "309/309 [==============================] - 44s 144ms/step - loss: 1.2242 - accuracy: 0.5855\n",
            "Epoch 6/200\n",
            "309/309 [==============================] - 45s 144ms/step - loss: 1.1967 - accuracy: 0.5904\n",
            "Epoch 7/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.1817 - accuracy: 0.5975\n",
            "Epoch 8/200\n",
            "309/309 [==============================] - 44s 144ms/step - loss: 1.1653 - accuracy: 0.6059\n",
            "Epoch 9/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.1682 - accuracy: 0.5979\n",
            "Epoch 10/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.1523 - accuracy: 0.6072\n",
            "Epoch 11/200\n",
            "309/309 [==============================] - 44s 142ms/step - loss: 1.1399 - accuracy: 0.6155\n",
            "Epoch 12/200\n",
            "309/309 [==============================] - 49s 158ms/step - loss: 1.1373 - accuracy: 0.6064\n",
            "Epoch 13/200\n",
            "309/309 [==============================] - 54s 173ms/step - loss: 1.1183 - accuracy: 0.6167\n",
            "Epoch 14/200\n",
            "309/309 [==============================] - 48s 155ms/step - loss: 1.1287 - accuracy: 0.6187\n",
            "Epoch 15/200\n",
            "309/309 [==============================] - 47s 151ms/step - loss: 1.1211 - accuracy: 0.6123\n",
            "Epoch 16/200\n",
            "309/309 [==============================] - 47s 153ms/step - loss: 1.1160 - accuracy: 0.6207\n",
            "Epoch 17/200\n",
            "309/309 [==============================] - 84s 273ms/step - loss: 1.1179 - accuracy: 0.6167\n",
            "Epoch 18/200\n",
            "309/309 [==============================] - 51s 165ms/step - loss: 1.1184 - accuracy: 0.6191\n",
            "Epoch 19/200\n",
            "309/309 [==============================] - 50s 162ms/step - loss: 1.1100 - accuracy: 0.6277\n",
            "Epoch 20/200\n",
            "309/309 [==============================] - 48s 154ms/step - loss: 1.1108 - accuracy: 0.6189\n",
            "Epoch 21/200\n",
            "309/309 [==============================] - 48s 155ms/step - loss: 1.1051 - accuracy: 0.6217\n",
            "Epoch 22/200\n",
            "309/309 [==============================] - 48s 154ms/step - loss: 1.1175 - accuracy: 0.6196\n",
            "Epoch 23/200\n",
            "309/309 [==============================] - 49s 159ms/step - loss: 1.0915 - accuracy: 0.6262\n",
            "Epoch 24/200\n",
            "309/309 [==============================] - 49s 158ms/step - loss: 1.0971 - accuracy: 0.6258\n",
            "Epoch 25/200\n",
            "309/309 [==============================] - 47s 151ms/step - loss: 1.1026 - accuracy: 0.6226\n",
            "Epoch 26/200\n",
            "309/309 [==============================] - 47s 151ms/step - loss: 1.1061 - accuracy: 0.6201\n",
            "Epoch 27/200\n",
            "309/309 [==============================] - 46s 148ms/step - loss: 1.1053 - accuracy: 0.6228\n",
            "Epoch 28/200\n",
            "309/309 [==============================] - 46s 148ms/step - loss: 1.0899 - accuracy: 0.6238\n",
            "Epoch 29/200\n",
            "309/309 [==============================] - 55s 179ms/step - loss: 1.0838 - accuracy: 0.6243\n",
            "Epoch 30/200\n",
            "309/309 [==============================] - 48s 155ms/step - loss: 1.0940 - accuracy: 0.6246\n",
            "Epoch 31/200\n",
            "309/309 [==============================] - 48s 155ms/step - loss: 1.0867 - accuracy: 0.6259\n",
            "Epoch 32/200\n",
            "309/309 [==============================] - 48s 155ms/step - loss: 1.0861 - accuracy: 0.6263\n",
            "Epoch 33/200\n",
            "309/309 [==============================] - 48s 155ms/step - loss: 1.0901 - accuracy: 0.6238\n",
            "Epoch 34/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0782 - accuracy: 0.6318\n",
            "Epoch 35/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0768 - accuracy: 0.6314\n",
            "Epoch 36/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0902 - accuracy: 0.6214\n",
            "Epoch 37/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0881 - accuracy: 0.6275\n",
            "Epoch 38/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0950 - accuracy: 0.6210\n",
            "Epoch 39/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0778 - accuracy: 0.6368\n",
            "Epoch 40/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0818 - accuracy: 0.6272\n",
            "Epoch 41/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0815 - accuracy: 0.6323\n",
            "Epoch 42/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0707 - accuracy: 0.6363\n",
            "Epoch 43/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0733 - accuracy: 0.6346\n",
            "Epoch 44/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0759 - accuracy: 0.6343\n",
            "Epoch 45/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0678 - accuracy: 0.6323\n",
            "Epoch 46/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0743 - accuracy: 0.6301\n",
            "Epoch 47/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0744 - accuracy: 0.6348\n",
            "Epoch 48/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0652 - accuracy: 0.6361\n",
            "Epoch 49/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0721 - accuracy: 0.6364\n",
            "Epoch 50/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0762 - accuracy: 0.6329\n",
            "Epoch 51/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0761 - accuracy: 0.6317\n",
            "Epoch 52/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0740 - accuracy: 0.6288\n",
            "Epoch 53/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0709 - accuracy: 0.6318\n",
            "Epoch 54/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0671 - accuracy: 0.6363\n",
            "Epoch 55/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0782 - accuracy: 0.6302\n",
            "Epoch 56/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0656 - accuracy: 0.6340\n",
            "Epoch 57/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0718 - accuracy: 0.6353\n",
            "Epoch 58/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0834 - accuracy: 0.6331\n",
            "Epoch 59/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0788 - accuracy: 0.6275\n",
            "Epoch 60/200\n",
            "309/309 [==============================] - 49s 157ms/step - loss: 1.0753 - accuracy: 0.6348\n",
            "Epoch 61/200\n",
            "309/309 [==============================] - 48s 156ms/step - loss: 1.0749 - accuracy: 0.6342\n",
            "Epoch 62/200\n",
            "309/309 [==============================] - 48s 154ms/step - loss: 1.0859 - accuracy: 0.6346\n",
            "Epoch 63/200\n",
            "309/309 [==============================] - 46s 150ms/step - loss: 1.0728 - accuracy: 0.6344\n",
            "Epoch 64/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0741 - accuracy: 0.6343\n",
            "Epoch 65/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0752 - accuracy: 0.6336\n",
            "Epoch 66/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0755 - accuracy: 0.6331\n",
            "Epoch 67/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0661 - accuracy: 0.6340\n",
            "Epoch 68/200\n",
            "309/309 [==============================] - 46s 148ms/step - loss: 1.0754 - accuracy: 0.6310\n",
            "Epoch 69/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0681 - accuracy: 0.6362\n",
            "Epoch 70/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0735 - accuracy: 0.6302\n",
            "Epoch 71/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0833 - accuracy: 0.6284\n",
            "Epoch 72/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0714 - accuracy: 0.6358\n",
            "Epoch 73/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0716 - accuracy: 0.6317\n",
            "Epoch 74/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0596 - accuracy: 0.6334\n",
            "Epoch 75/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0688 - accuracy: 0.6328\n",
            "Epoch 76/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0569 - accuracy: 0.6405\n",
            "Epoch 77/200\n",
            "309/309 [==============================] - 46s 149ms/step - loss: 1.0602 - accuracy: 0.6339\n",
            "Epoch 78/200\n",
            "309/309 [==============================] - 47s 153ms/step - loss: 1.0702 - accuracy: 0.6360\n",
            "Epoch 79/200\n",
            "309/309 [==============================] - 44s 142ms/step - loss: 1.0757 - accuracy: 0.6317\n",
            "Epoch 80/200\n",
            "309/309 [==============================] - 44s 142ms/step - loss: 1.0685 - accuracy: 0.6292\n",
            "Epoch 81/200\n",
            "309/309 [==============================] - 44s 142ms/step - loss: 1.0573 - accuracy: 0.6325\n",
            "Epoch 82/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.0660 - accuracy: 0.6359\n",
            "Epoch 83/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.0708 - accuracy: 0.6326\n",
            "Epoch 84/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.0643 - accuracy: 0.6341\n",
            "Epoch 85/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.0626 - accuracy: 0.6368\n",
            "Epoch 86/200\n",
            "309/309 [==============================] - 44s 143ms/step - loss: 1.0738 - accuracy: 0.6323\n",
            "Epoch 87/200\n",
            "309/309 [==============================] - 44s 141ms/step - loss: 1.0668 - accuracy: 0.6366\n",
            "Epoch 88/200\n",
            "309/309 [==============================] - 44s 142ms/step - loss: 1.0575 - accuracy: 0.6389\n",
            "Epoch 89/200\n",
            "309/309 [==============================] - 44s 141ms/step - loss: 1.0553 - accuracy: 0.6378\n",
            "Epoch 90/200\n",
            "309/309 [==============================] - 44s 141ms/step - loss: 1.0666 - accuracy: 0.6358\n",
            "Epoch 91/200\n",
            "309/309 [==============================] - 44s 141ms/step - loss: 1.0700 - accuracy: 0.6330\n",
            "Epoch 92/200\n",
            "309/309 [==============================] - 44s 141ms/step - loss: 1.0685 - accuracy: 0.6336\n",
            "Epoch 93/200\n",
            "309/309 [==============================] - 44s 142ms/step - loss: 1.0730 - accuracy: 0.6363\n",
            "Epoch 94/200\n",
            "309/309 [==============================] - 44s 142ms/step - loss: 1.0660 - accuracy: 0.6360\n",
            "Epoch 95/200\n",
            " 58/309 [====>.........................] - ETA: 35s - loss: 1.0850 - accuracy: 0.6234"
          ]
        }
      ],
      "source": [
        "# TODO 6 (required) - fit model, you decide how many epochs\n",
        "# note: you can get the number of training samples with training_gen.n\n",
        "# and the number of validation samples with validation_gen.n\n",
        "\n",
        "# at most 200 epochs\n",
        "n_epochs = 200\n",
        "hist = model.fit(x=training_gen,epochs=n_epochs,batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpALQQS5PnVG"
      },
      "source": [
        "Plot the training history. In one subplot, show loss vs. epoch for the training and validation sets on the same plot. In the second subplot, show accuracy vs. epoch for the training and validation sets on the same plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuN1eFlnYliH"
      },
      "source": [
        "Make sure you show the training history from the *beginning* of training. (If you re-run the `hist = fit(...)` command, you'll overwrite the previous history and lose the training history from the beginning, so be careful not to do that in your final submission!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHRrGZuxPqdt"
      },
      "outputs": [],
      "source": [
        "# TODO 7 (required) - plot training history\n",
        "\n",
        "# loss vs epoch\n",
        "plt.semilogy(hist.history['loss'], label='Training loss')\n",
        "plt.semilogy(hist.history['val_loss'], label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# accuracy vs epoch\n",
        "plt.figure(figsize=(7,3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "train_acc = hist.history['accuracy'];\n",
        "val_acc = hist.history['val_accuracy'];\n",
        "\n",
        "nepochs = len(train_acc);\n",
        "sns.lineplot(x=np.arange(1,nepochs+1), y=train_acc, label='Training accuracy');\n",
        "sns.lineplot(x=np.arange(1,nepochs+1), y=val_acc, label='Validation accuracy');\n",
        "plt.xlabel('Epoch');\n",
        "plt.ylabel('Accuracy');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zcjS3zLOaj8"
      },
      "source": [
        "## Fine-tune model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIvVbZ5jOeS_"
      },
      "source": [
        "Next, we are going to \"un-freeze\" the later layers of the model, and train it for a few more epochs on our data (with a smaller learning rate), so that it is better suited for our specific classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpKQzY0iQwMf"
      },
      "source": [
        "Note that you should *not* create a new model. We're just going to continue training the model we already started training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu5MMmW3Tqu9"
      },
      "source": [
        "First, we will un-freeze the base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zvH5cbUTlce"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE1jagiiTsdb"
      },
      "source": [
        "Then, we will re-freeze only the first layers of the base model. In the cell below, I have left the last five layers, which includes the last set of convolutional filters, unfrozen. But you are free to change this number if you think you will get better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Nzlw-jRF6L"
      },
      "outputs": [],
      "source": [
        "# TODO 8 (optional) - you can decide how many layers to unfreeze\n",
        "\n",
        "for layer in base_model.layers[:-5]:\n",
        "\tlayer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg5eufJMRX26"
      },
      "source": [
        "The output of the following cell will indicate which layers in the base model are trainable, and which are not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXj5rAKfRa_H"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "  print(\"{}: {}\".format(layer, layer.trainable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei-B5vBqUYAb"
      },
      "source": [
        "Also, the model summary should now show more trainable parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YASFan3RqYD"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOZYyBqiUb1t"
      },
      "source": [
        "Re-compile your model. Don't forget to use a smaller learning rate for fine-tuning! We don't want to make major changes to the last feature extraction layers, we just want to adjust them a little bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ZeQWf0RxOV"
      },
      "outputs": [],
      "source": [
        "# TODO 9 (required) - re-compile model\n",
        "# use a smaller learning rate for fine-tuning\n",
        "\n",
        "# model.compile(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6QKGEeNUrkW"
      },
      "source": [
        "Fit your model (with some newly unfrozen layers) for some more epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGtHqsZuU0M2"
      },
      "outputs": [],
      "source": [
        "# TODO 10 (required) - fit model, you decide how many epochs\n",
        "# note: you can get the number of training samples with training_gen.n\n",
        "# and the number of validation samples with validation_gen.n\n",
        "\n",
        "# n_epochs_fine = ...\n",
        "# hist_fine = model.fit(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw5OSPJAVIc-"
      },
      "source": [
        "Plot the complete training history. In one subplot, show loss vs. epoch for the training and validation sets on the same plot. In the second subplot, show accuracy vs. epoch for the training and validation sets on the same plot. Draw a vertical line demarcating the first stage of training (training the classification head) and the second stage of training (fine tuning).\n",
        "\n",
        "(You can refer to this week's demo notebook for an example.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to2F-yESYwdN"
      },
      "source": [
        "Make sure you show the training history from the *beginning* of training. (If you re-run the `hist_fine = fit(...)` command, you'll overwrite the previous history and lose the training history from the beginning, so be careful not to do that in your final submission!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBnfw72ZVYNz"
      },
      "outputs": [],
      "source": [
        "# TODO 11 (required) - plot training history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZWO3ILaOjsr"
      },
      "source": [
        "## Evaluate model performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXMmLhC0XgNq"
      },
      "source": [
        "Once you have finished training your model, use `evaluate` to get the model performance - loss and accuracy - on the evaluation set. (You should not use the evaluation set at all prior to this step.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioyIKe0DXUz7"
      },
      "outputs": [],
      "source": [
        "model.evaluate(evaluation_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkMrM7tLbv4e"
      },
      "source": [
        "We can also create a confusion matrix, to see which which classes are most often confused with one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bytoNNyLeJjL"
      },
      "outputs": [],
      "source": [
        "y_pred_prob = model.predict(evaluation_gen)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzHPEM9oYRD6"
      },
      "outputs": [],
      "source": [
        "cm = pd.crosstab(evaluation_gen.classes, y_pred,\n",
        "                               rownames=['Actual'], colnames=['Predicted'],\n",
        "                               normalize='index')\n",
        "p = plt.figure(figsize=(10,10));\n",
        "p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False);\n",
        "p = plt.xticks(ticks=np.arange(0,11), labels=classes, rotation=45)\n",
        "p = plt.yticks(ticks=np.arange(0,11), labels=classes, rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3iA5I5FaeBJ"
      },
      "source": [
        "## Save your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzxuNXhGai6r"
      },
      "source": [
        "Use the following cell to save your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvKTAkDladKj"
      },
      "outputs": [],
      "source": [
        "model.save(\"model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p12O802axbj"
      },
      "source": [
        "Then, download it for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRPAgf57bKjF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-O0yJTOlEq"
      },
      "source": [
        "## Use model on your own custom image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_mKCQ0hWwjV"
      },
      "source": [
        "Take your own photograph of a food item in one of the 11 classes. Upload it to Colab, load it as a numpy array, and process the image (resize it, and scale so that each pixel takes on a value from 0 to 1).\n",
        "\n",
        "Then, use the fitted model to get the per-class probabilities for each class.\n",
        "\n",
        "\n",
        "Plot your image, and a bar plot showing the probability for the 5 most likely classes according to the model.\n",
        "\n",
        "(You can refer to the \"Classify with MobileNetV2\" section of this week's notebook for an example.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Cg07rZXNVR"
      },
      "outputs": [],
      "source": [
        "# TODO 12 (required) - show example of model use on a new image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lkVkl-phXxg"
      },
      "source": [
        "## Describe your model - TODO 13 (required)\n",
        "\n",
        "In the following cell, describe your final model and your results, by answering the following questions.\n",
        "\n",
        "* What base model did you use? How many parameters does this base model have?\n",
        "* Describe the specific transformations you used to create an augmented data set.\n",
        "* For how many epochs did you train your classification head, and with what optimizer, learning rate, and batch size? What was the validation accuracy at the end of this training stage?\n",
        "* Which layers did you un-freeze for fine-tuning, how many epochs did you fine-tune for, and what optimizer, learning rate, and batch size did you use for fine-tuning? What was the validation accuracy at the end of this training stage?\n",
        "* What was your final accuracy on the evaluation set?\n",
        "* Did your model correctly predict the class of your custom test image?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3lkVkl-phXxg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
